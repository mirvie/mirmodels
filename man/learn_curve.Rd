% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learning-curve.R
\name{learn_curve}
\alias{learn_curve}
\title{Get the learning curve of a model as training data quantity increases.}
\usage{
learn_curve(
  model_evaluate,
  training_data,
  outcome,
  testing_data = NULL,
  testing_frac = NULL,
  training_fracs = seq(0.1, 1, by = 0.1),
  repeats = 1,
  strata = NULL,
  n_cores = 1
)
}
\arguments{
\item{model_evaluate}{A function with exactly two arguments: \code{training_data}
and \code{testing_data} that trains the model of choice on \code{training_data} and
then produces predictions on \code{testing_data}, finally evaluating those
predictions and outputting a length two numeric vector with names "cv" and
"test" giving the cross-validation and test scores from the evaluation.}

\item{training_data}{A data frame. Subsets of this will be used for training.
If \code{testing_data} is \code{NULL} and \code{testing_frac} is not, this will be split
into training and testing sets, with \code{testing_frac} used for testing.}

\item{outcome}{A string. The name of the outcome variable. This must be a
column in \code{training_data}.}

\item{testing_data}{A data frame. The trained models will all be tested
against this constant test set.}

\item{testing_frac}{A numeric vector with values between 0 and 1/3.The
fraction of \code{training_data} to use for the test set. This can only be used
if \code{testing_data} is \code{NULL}. To try many different fractions, specify all
of them as a numeric vector.}

\item{training_fracs}{A numeric vector. Fractions of the training data to
use. This must be a positive, increasing vector of real numbers ending in
1.}

\item{repeats}{A positive integer. The number of times to repeat the sampling
for each proportion in \code{testing_frac}. This can be greater than 1 only if
\code{testing_data} is \code{NULL} and \code{testing_frac} is not \code{NULL}. For each repeat,
a different subsetting of \code{testing_data} remains takes place.}

\item{strata}{A string. Variable to stratify on when splitting data.}

\item{n_cores}{A positive integer. The cross-validation can optionally be
done in parallel. Specify the number of cores for parallel processing here.}
}
\value{
A data frame with the following columns.
\itemize{
\item \code{rep}: The repeat number.
\item \code{testing_frac}: The fraction of \code{training_data} that is set aside for
testing. If the \code{testing_data} argument is specified, \code{testing_frac} will
be 0, because none of \code{training_data} is set aside for testing.
\item \code{training_frac}: The fraction of the (post train/test split)training data
used for learning.
\item \code{testing_indices}: The row indices of the \code{training_data} argument that
were set aside for testing. If \code{testing_data} is specified (and hence
none of \code{training_data} needs to be set aside for testing, this will be a
vector of \code{NA}s with length equal to the number of rows in
\code{testing_data}.
\item \code{training_indices}: The row indices of the \code{training_data} that were used
for learning.
\item \code{cv}: The cross-validation score.
\item \code{test}: The test score.
}
}
\description{
Given a training and test set, fit a model on increasing fractions of the
training set, up to the full set, with a constant test set per repeat (each
repeat will have a different test set). The default is to use 10\%, 20\%, 30\%,
. . ., 90\%, 100\%. Care is taken to make sure each fraction is a subset of the
last e.g. all samples present in the 10\% will be present in the 20\% to
simulate the addition of more data, as opposed to a random sample of more
data. Optionally, you can pass all of your data in as the \code{training_data} and
then get the function to do the splitting for you.
}
\examples{
data("BostonHousing", package = "mlbench")
bh <- dplyr::select_if(BostonHousing, is.numeric)
model_evaluate <- function(training_data, testing_data) {
  trained_mod <- lm(medv ~ ., training_data)
  training_preds <- predict(trained_mod, newdata = training_data)
  preds <- predict(trained_mod, newdata = testing_data)
  c(
    train = yardstick::mae_vec(training_data$medv, training_preds),
    test = yardstick::mae_vec(testing_data$medv, preds)
  )
}
mlc <- mlc0 <- suppressWarnings(
  learn_curve(model_evaluate, bh, "medv",
    training_fracs = c(seq(0.1, 0.7, 0.2), 0.85),
    testing_frac = c(0.25, 0.5), repeats = 8,
    strata = "medv", n_cores = 4
  )
)
}
\seealso{
\code{\link[=autoplot.mirvie_learning_curve]{autoplot.mirvie_learning_curve()}}
}
