% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lm.R
\name{train_lm}
\alias{train_lm}
\title{Train a Lasso linear model.}
\usage{
train_lm(
  training_data,
  outcome,
  metric = c("rmse", "mae"),
  na_action = c("medianimpute", "knnimpute"),
  lambda = NULL,
  cv_nfolds = 10,
  id_col = NULL,
  strata = NULL,
  selection_method = "Breiman",
  include_nullmod = TRUE,
  err_if_nullmod = FALSE,
  warn_if_nullmod = TRUE,
  n_cores = 1
)
}
\arguments{
\item{training_data}{A data frame. The data used to train the model.}

\item{outcome}{A string. The name of the outcome variable. This must be a
column in \code{training_data}.}

\item{metric}{A string. \code{"rmse"} or \code{"mae"}.}

\item{na_action}{A string. How to impute missing data in explanatory
variables\code{"medianimpute"} or \code{"knnimpute"}. See
\code{\link[recipes:step_impute_median]{recipes::step_medianimpute()}} and \code{\link[recipes:step_impute_knn]{recipes::step_knnimpute()}}. Default is
\code{"medianimpute"}.}

\item{lambda}{A numeric vector. Optional. A grid of lambdas for tuning the
Lasso. If you leave this as \code{NULL}, recommended, a sensible grid is chosen
for you.}

\item{cv_nfolds}{A positive integer. The number of folds for
cross-validation.}

\item{id_col}{A string. If there is a sample identifier column, specify it
here to tell the model not to use it as a predictor.}

\item{strata}{A string. Variable to stratify on when splitting for
cross-validation.}

\item{selection_method}{A string. How to select the best model. There are two
options: "Breiman" and "absolute". "absolute" selects the best model by
selecting the model with the best mean performance according to the chosen
metric. "Breiman" selects the simplest model that comes within one standard
deviation of the best score. The idea being that simple models generalize
better, so it's better to select a simple model that had near-best
performance.}

\item{include_nullmod}{A bool. Include the null model (predicts mean or most
common class every time) in the model comparison? This is recommended. If
the null model comes within a standard deviation of the otherwise best
model, the null model is chosen instead.}

\item{err_if_nullmod}{A bool. If the null model is chosen, throw an error
rather than returning the null model.}

\item{warn_if_nullmod}{A bool. Warn if returning the null model?}

\item{n_cores}{A positive integer. The cross-validation can optionally be
done in parallel. Specify the number of cores for parallel processing here.}
}
\value{
A \link[parsnip:model_fit]{parsnip::model_fit} object. To use this fitted model \code{mod} to make
predictions on some new data \code{df_new}, use
\code{predict(mod, new_data = df_new)}.
}
\description{
Train a Lasso linear model. The training routine automatically selects the
best lambda parameter using \code{\link[glmnet:cv.glmnet]{glmnet::cv.glmnet()}}.
}
\examples{
iris_data <- janitor::clean_names(datasets::iris)
iris_data_split <- rsample::initial_split(iris_data, strata = species)
mod <- train_lm(
  training_data = rsample::training(iris_data_split),
  outcome = "petal_length",
  metric = "mae",
  n_cores = 5
)
preds <- predict(mod, new_data = rsample::testing(iris_data_split))
dplyr::bind_cols(preds,
  truth = rsample::testing(iris_data_split)$petal_length
)
yardstick::mae_vec(
  truth = rsample::testing(iris_data_split)$petal_length,
  estimate = preds[[1]]
)
}
\seealso{
Other model trainers: 
\code{\link{train_gbm}()},
\code{\link{train_glm}()}
}
\concept{model trainers}
