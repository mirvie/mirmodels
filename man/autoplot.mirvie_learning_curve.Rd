% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learning-curve.R
\name{autoplot.mirvie_learning_curve}
\alias{autoplot.mirvie_learning_curve}
\title{Produce a learning curve plot from a
\link[=new_mirvie_learning_curve]{mirvie_learning_curve} object.}
\usage{
\method{autoplot}{mirvie_learning_curve}(object, metric = NULL, smooth = FALSE, meansd = FALSE, ...)
}
\arguments{
\item{object}{A \link[=new_mirvie_learning_curve]{mirvie_learning_curve} object
(i.e. the output of a call to \code{\link[=learn_curve]{learn_curve()}}).}

\item{metric}{A string. The metric used to evaluate the performance.}

\item{smooth}{A flag. Use a loess smoothed line instead of joining the dots?}

\item{meansd}{A flag. If there are multiple repeats, rather than plotting all
of them, plot means with standard deviation error bars?}

\item{...}{Arguments passed to \code{\link[ggplot2:autoplot]{ggplot2::autoplot()}}. Safe to ignore.}
}
\value{
A \code{\link[ggplot2:ggplot]{ggplot2::ggplot()}}.
}
\description{
This is a method for \code{\link[ggplot2:autoplot]{ggplot2::autoplot()}}.
}
\examples{
data("BostonHousing", package = "mlbench")
bh <- dplyr::select_if(BostonHousing, is.numeric)
model_evaluate <- function(training_data, testing_data) {
  trained_mod <- lm(medv ~ ., training_data)
  training_preds <- predict(trained_mod, newdata = training_data)
  preds <- predict(trained_mod, newdata = testing_data)
  c(
    train = yardstick::mae_vec(training_data$medv, training_preds),
    test = yardstick::mae_vec(testing_data$medv, preds)
  )
}
mlc <- mlc0 <- suppressWarnings(
  learn_curve(model_evaluate, bh, "medv",
    training_fracs = c(seq(0.1, 0.7, 0.2), 0.85),
    testing_frac = c(0.25, 0.5), repeats = 8,
    strata = "medv"
  )
)
suppressWarnings(print(autoplot(mlc, metric = "mae")))
suppressWarnings(print(autoplot(mlc, metric = "mae", smooth = TRUE)))
suppressWarnings(print(autoplot(mlc, metric = "mae", meansd = TRUE)))
suppressWarnings(
  print(autoplot(mlc, metric = "mae", smooth = TRUE, meansd = TRUE))
)
mlc <- dplyr::filter(mlc0, testing_frac == 0.25)
suppressWarnings(print(autoplot(mlc, metric = "mae")))
suppressWarnings(print(autoplot(mlc, metric = "mae", smooth = TRUE)))
suppressWarnings(print(autoplot(mlc, metric = "mae", meansd = TRUE)))
suppressWarnings(
  print(autoplot(mlc, metric = "mae", smooth = TRUE, meansd = TRUE))
)
mlc <- dplyr::filter(mlc0, rep == 1)
suppressWarnings(print(autoplot(mlc, metric = "mae")))
suppressWarnings(print(autoplot(mlc, metric = "mae", smooth = TRUE)))
mlc <- dplyr::filter(mlc0, rep == 1, testing_frac == 0.25)
suppressWarnings(print(autoplot(mlc, metric = "mae")))
suppressWarnings(print(autoplot(mlc, metric = "mae", smooth = TRUE)))
bh_split <- rsample::initial_split(bh, strata = medv)
bh_training <- rsample::training(bh_split)
bh_testing <- rsample::testing(bh_split)
mlc <- learn_curve(model_evaluate,
  training_data = bh_training,
  outcome = "medv", testing_data = bh_testing,
  strata = "medv"
)
suppressWarnings(print(autoplot(mlc)))
suppressWarnings(print(autoplot(mlc, smooth = TRUE)))
}
