% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/grid.R
\name{train_on_grid}
\alias{train_on_grid}
\title{Train several models with different hyperparameters and select the best one.}
\usage{
train_on_grid(
  mod_spec,
  hyper_param_grid,
  mod_rec,
  training_data,
  outcome,
  cv_nfolds,
  cv_nreps = 1,
  strata = NULL,
  id_col = NULL,
  metric,
  selection_method = "Breiman",
  simplicity_params = NULL,
  include_nullmod = TRUE,
  err_if_nullmod = FALSE,
  warn_if_nullmod = TRUE,
  n_cores = 1
)
}
\arguments{
\item{mod_spec}{A parsnip model specification. It must include the model mode
and engine. See \code{\link[parsnip:set_args]{parsnip::set_mode()}} and \code{\link[parsnip:set_engine]{parsnip::set_engine()}}.}

\item{hyper_param_grid}{A data frame with one row per hyperparameter
combination. The column names give the hyper parameter names. Can
optionally be passed as a list which is made into a tibble by
\code{\link[tidyr:expand_grid]{tidyr::expand_grid()}}.}

\item{mod_rec}{The recipe for preparing the data for this model. See
\code{\link[recipes:recipe]{recipes::recipe()}}.}

\item{training_data}{A data frame. The data used to train the model.}

\item{outcome}{A string. The name of the outcome variable. This must be a
column in \code{training_data}.}

\item{cv_nfolds}{A positive integer. The number of folds for
cross-validation.}

\item{cv_nreps}{A positive integer. The number of repeated rounds in the
cross-validation.}

\item{strata}{A string. Variable to stratify on when splitting for
cross-validation.}

\item{id_col}{A string. If there is a sample identifier column, specify it
here to tell the model not to use it as a predictor.}

\item{metric}{A string. The metric to use to evaluate the models and select
the best one. Common choices are \code{"rmse"}, \code{"mae"}, \code{"roc_auc"},
\code{"accuracy"}, \code{"mn_log_loss"}. This should be a metric that is available in
the \code{yardstick} package, but use e.g. \code{"mae"} and not \code{"yardstick::mae"} in
this argumentIf you specify this as a multi-element character vector, the first element will be used to select the best model; subsequent metrics will also be reported for that model in the \code{cv_performance} attribute of the the returned object.)`}

\item{selection_method}{A string. How to select the best model. There are two
options: "Breiman" and "absolute". "absolute" selects the best model by
selecting the model with the best mean performance according to the chosen
metric. "Breiman" selects the simplest model that comes within one standard
deviation of the best score. The idea being that simple models generalize
better, so it's better to select a simple model that had near-best
performance.}

\item{simplicity_params}{A character vector. For \code{selection_method = "Breiman"}. These are passed directly to \code{\link[tune:show_best]{tune::select_by_one_std_err()}}
and used to sort \code{hyper_param_grid} by simplicity. To sort descending, put
a minus in front of the parameter. For example, to sort ascending on "x"
and then descending on "y", use \code{simplicity_params = c("x", "-y")}. See
\code{\link[tune:show_best]{tune::select_by_one_std_err()}} for details.}

\item{include_nullmod}{A bool. Include the null model (predicts mean or most
common class every time) in the model comparison? This is recommended. If
the null model comes within a standard deviation of the otherwise best
model, the null model is chosen instead.}

\item{err_if_nullmod}{A bool. If the null model is chosen, throw an error
rather than returning the null model.}

\item{warn_if_nullmod}{A bool. Warn if returning the null model?}

\item{n_cores}{A positive integer. The cross-validation can optionally be
done in parallel. Specify the number of cores for parallel processing here.}
}
\value{
A \code{parsnip} \code{model_fit} object with a \code{predict()} method and \code{recipe}
and \code{cv_performance} attributes.
}
\description{
Tune a model based on a provided hyperparameter grid by cross-validation.
Select the best one, optionally including the null model in the comparison.
}
